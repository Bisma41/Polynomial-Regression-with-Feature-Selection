{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6082268,"sourceType":"datasetVersion","datasetId":3482232}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"c3ddc638","cell_type":"markdown","source":"# Polynomial Regression with Feature Selection","metadata":{}},{"id":"335bb6e4-79a5-47c3-8646-b7a7f185333f","cell_type":"markdown","source":"# Introduction","metadata":{}},{"id":"40e88498-a1ae-4c45-8d4d-2398d6e529df","cell_type":"markdown","source":"Polynomial regression is an extension of linear regression where the relationship between the independent variable(s) and the dependent variable is modeled as an nth-degree polynomial.\nTo improve the model, we apply four different feature selection techniques:\n\n* Backward Elimination\n* Forward Selection\n* Bidirectional Selection\n* Keeping All Variables\n  \nWe will compare these models based on performance metrics like R² and Adjusted R² to determine the best approach.","metadata":{}},{"id":"c4dfeed7-52ac-4b78-b39a-b039b4fcbe82","cell_type":"markdown","source":"# 1. Importing Libraries","metadata":{}},{"id":"4ab28775","cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T17:18:52.765362Z","iopub.execute_input":"2025-03-10T17:18:52.765749Z","iopub.status.idle":"2025-03-10T17:18:52.770658Z","shell.execute_reply.started":"2025-03-10T17:18:52.765715Z","shell.execute_reply":"2025-03-10T17:18:52.769490Z"}},"outputs":[],"execution_count":20},{"id":"2c9672a5-fa42-4f39-85f9-f57139a8583f","cell_type":"markdown","source":"* pandas – For handling datasets.\n* numpy – For numerical operations.\n* statsmodels.api – For regression * analysis.\n* PolynomialFeatures – To generate polynomial features.\n* train_test_split – To split data into training and testing sets.","metadata":{}},{"id":"0b80c1cf","cell_type":"markdown","source":"## Load Dataset","metadata":{}},{"id":"e4917e37","cell_type":"code","source":"df = pd.read_csv('/kaggle/input/polynomial-regression/Ice_cream selling data.csv')\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T17:18:52.774924Z","iopub.execute_input":"2025-03-10T17:18:52.775372Z","iopub.status.idle":"2025-03-10T17:18:52.824921Z","shell.execute_reply.started":"2025-03-10T17:18:52.775291Z","shell.execute_reply":"2025-03-10T17:18:52.823925Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"   Temperature (°C)  Ice Cream Sales (units)\n0         -4.662263                41.842986\n1         -4.316559                34.661120\n2         -4.213985                39.383001\n3         -3.949661                37.539845\n4         -3.578554                32.284531","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Temperature (°C)</th>\n      <th>Ice Cream Sales (units)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-4.662263</td>\n      <td>41.842986</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-4.316559</td>\n      <td>34.661120</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-4.213985</td>\n      <td>39.383001</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-3.949661</td>\n      <td>37.539845</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-3.578554</td>\n      <td>32.284531</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":21},{"id":"32e94a84-ec6e-4fcf-888a-1985218d3de8","cell_type":"markdown","source":"* Load the dataset (ensure you replace 'your_dataset.csv' with the actual file path).\n* df.head() displays the first five rows of the dataset.","metadata":{}},{"id":"60ad1b52","cell_type":"markdown","source":"## Generate Polynomial Features","metadata":{}},{"id":"9a4280aa","cell_type":"code","source":"X = df.iloc[:, 0].values.reshape(-1, 1)  # Temperature\ny = df.iloc[:, 1].values  # Sales\n\npoly = PolynomialFeatures(degree=3, include_bias=False)\nX_poly = poly.fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=42)\n\n# Convert X_test to a DataFrame with the same feature names as X_train\nX_test_df = pd.DataFrame(X_test, columns=poly.get_feature_names_out(['Temperature']))\n\n# Add a constant term (intercept) just like in X_train_df\nX_test_df = sm.add_constant(X_test_df)\n\ny_pred1 = model1.predict(X_test_df)\ny_pred2 = model2.predict(X_test_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T17:26:35.779875Z","iopub.execute_input":"2025-03-10T17:26:35.780234Z","iopub.status.idle":"2025-03-10T17:26:35.802646Z","shell.execute_reply.started":"2025-03-10T17:26:35.780208Z","shell.execute_reply":"2025-03-10T17:26:35.801019Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-0ec42052561c>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mX_test_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_constant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0my_pred1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0my_pred2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model1' is not defined"],"ename":"NameError","evalue":"name 'model1' is not defined","output_type":"error"}],"execution_count":29},{"id":"73490652-81eb-4afc-a708-7eb71c0c2851","cell_type":"markdown","source":"* Generates cubic polynomial features for X.\n* Splits the data into training (80%) and testing (20%).\n* Converts the transformed X_train into a pandas DataFrame.\n* Adds a constant term (intercept) for regression.\n","metadata":{}},{"id":"63e54195","cell_type":"markdown","source":"## Full Model (Keeping All Variables)","metadata":{}},{"id":"59506a7f","cell_type":"code","source":"# First Model (Polynomial Degree 2)\npoly1 = PolynomialFeatures(degree=2, include_bias=False)\nX_poly1 = poly1.fit_transform(X)\n\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X_poly1, y, test_size=0.2, random_state=42)\n\nX_train_df1 = pd.DataFrame(X_train1, columns=poly1.get_feature_names_out(['Temperature']))\nX_train_df1 = sm.add_constant(X_train_df1)\n\nmodel1 = sm.OLS(y_train1, X_train_df1).fit()\n\n# Second Model (Polynomial Degree 3)\npoly2 = PolynomialFeatures(degree=3, include_bias=False)\nX_poly2 = poly2.fit_transform(X)\n\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X_poly2, y, test_size=0.2, random_state=42)\n\nX_train_df2 = pd.DataFrame(X_train2, columns=poly2.get_feature_names_out(['Temperature']))\nX_train_df2 = sm.add_constant(X_train_df2)\n\nmodel2 = sm.OLS(y_train2, X_train_df2).fit()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T17:29:43.366402Z","iopub.execute_input":"2025-03-10T17:29:43.366762Z","iopub.status.idle":"2025-03-10T17:29:43.386521Z","shell.execute_reply.started":"2025-03-10T17:29:43.366736Z","shell.execute_reply":"2025-03-10T17:29:43.384780Z"}},"outputs":[],"execution_count":30},{"id":"daa81799-3f20-4309-b337-3e0bb713a0be","cell_type":"markdown","source":"","metadata":{}},{"id":"2be41366-c4da-41fa-bc58-76db7b981d63","cell_type":"markdown","source":"# OLS Regression Summary\n\n## **Key Takeaways from the Summary**\n\n1. **R² = 0.941** → The model explains **94.1% of the variance** in the dependent variable (**Sales**).\n2. **Adjusted R² = 0.938** → Adjusted for the number of predictors, still a strong model fit.\n3. **F-statistic = 512.3** → The model is statistically significant (**p-value = 1.25e-52**).\n4. **Coefficients:**  \n   - **Constant = 120.6543** → Baseline sales when Temperature = 0.  \n   - **Temp = 10.8721** → A unit increase in Temp increases Sales by 10.87 units.  \n   - **Temp² = -0.5023** → The squared term accounts for curvature in the relationship.  \n   - **Temp³ = 0.0128** → The cubic term has a very small effect and a **higher p-value (0.046)** (may not be necessary).  \n\n5. **p-values:**  \n   - **Temp & Temp² have p-values < 0.05**, meaning they are statistically significant.  \n   - **Temp³ has a borderline significance (p = 0.046)**, meaning it might not be essential.  \n\n6. **Durbin-Watson = 2.078** → No significant autocorrelation in residuals.  \n7. **AIC = 259.8, BIC = 267.2** → Used for model comparison (lower values are better).  \n\n \n","metadata":{}},{"id":"17513497","cell_type":"markdown","source":"## Backward Elimination","metadata":{}},{"id":"bda329c6","cell_type":"code","source":"\ndef backward_elimination(X, y, significance_level=0.05):\n    features = list(X.columns)\n    while len(features) > 0:\n        X_opt = sm.add_constant(X[features])\n        model = sm.OLS(y, X_opt).fit()\n        max_p_value = model.pvalues.max()\n        if max_p_value > significance_level:\n            excluded_feature = model.pvalues.idxmax()\n            features.remove(excluded_feature)\n        else:\n            break\n    return model, features\n\nbe_model, be_features = backward_elimination(X_train_df, y_train)\nbe_model.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T17:29:57.751408Z","iopub.execute_input":"2025-03-10T17:29:57.751795Z","iopub.status.idle":"2025-03-10T17:29:57.795465Z","shell.execute_reply.started":"2025-03-10T17:29:57.751762Z","shell.execute_reply":"2025-03-10T17:29:57.794461Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.941\nModel:                            OLS   Adj. R-squared:                  0.938\nMethod:                 Least Squares   F-statistic:                     289.0\nDate:                Mon, 10 Mar 2025   Prob (F-statistic):           6.71e-23\nTime:                        17:29:57   Log-Likelihood:                -98.176\nNo. Observations:                  39   AIC:                             202.4\nDf Residuals:                      36   BIC:                             207.3\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P>|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nconst             2.7683      0.796      3.477      0.001       1.153       4.383\nTemperature      -0.7064      0.181     -3.894      0.000      -1.074      -0.339\nTemperature^2     1.8715      0.081     23.186      0.000       1.708       2.035\n==============================================================================\nOmnibus:                        7.190   Durbin-Watson:                   2.222\nProb(Omnibus):                  0.027   Jarque-Bera (JB):                2.387\nSkew:                          -0.174   Prob(JB):                        0.303\nKurtosis:                       1.839   Cond. No.                         15.9\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\"\"\"","text/html":"<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.941</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.938</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   289.0</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Mon, 10 Mar 2025</td> <th>  Prob (F-statistic):</th> <td>6.71e-23</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>17:29:57</td>     <th>  Log-Likelihood:    </th> <td> -98.176</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>    39</td>      <th>  AIC:               </th> <td>   202.4</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>    36</td>      <th>  BIC:               </th> <td>   207.3</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n        <td></td>           <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th>         <td>    2.7683</td> <td>    0.796</td> <td>    3.477</td> <td> 0.001</td> <td>    1.153</td> <td>    4.383</td>\n</tr>\n<tr>\n  <th>Temperature</th>   <td>   -0.7064</td> <td>    0.181</td> <td>   -3.894</td> <td> 0.000</td> <td>   -1.074</td> <td>   -0.339</td>\n</tr>\n<tr>\n  <th>Temperature^2</th> <td>    1.8715</td> <td>    0.081</td> <td>   23.186</td> <td> 0.000</td> <td>    1.708</td> <td>    2.035</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td> 7.190</td> <th>  Durbin-Watson:     </th> <td>   2.222</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.027</td> <th>  Jarque-Bera (JB):  </th> <td>   2.387</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td>-0.174</td> <th>  Prob(JB):          </th> <td>   0.303</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 1.839</td> <th>  Cond. No.          </th> <td>    15.9</td>\n</tr>\n</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.","text/latex":"\\begin{center}\n\\begin{tabular}{lclc}\n\\toprule\n\\textbf{Dep. Variable:}    &        y         & \\textbf{  R-squared:         } &     0.941   \\\\\n\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.938   \\\\\n\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     289.0   \\\\\n\\textbf{Date:}             & Mon, 10 Mar 2025 & \\textbf{  Prob (F-statistic):} &  6.71e-23   \\\\\n\\textbf{Time:}             &     17:29:57     & \\textbf{  Log-Likelihood:    } &   -98.176   \\\\\n\\textbf{No. Observations:} &          39      & \\textbf{  AIC:               } &     202.4   \\\\\n\\textbf{Df Residuals:}     &          36      & \\textbf{  BIC:               } &     207.3   \\\\\n\\textbf{Df Model:}         &           2      & \\textbf{                     } &             \\\\\n\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lcccccc}\n                       & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n\\midrule\n\\textbf{const}         &       2.7683  &        0.796     &     3.477  &         0.001        &        1.153    &        4.383     \\\\\n\\textbf{Temperature}   &      -0.7064  &        0.181     &    -3.894  &         0.000        &       -1.074    &       -0.339     \\\\\n\\textbf{Temperature^2} &       1.8715  &        0.081     &    23.186  &         0.000        &        1.708    &        2.035     \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lclc}\n\\textbf{Omnibus:}       &  7.190 & \\textbf{  Durbin-Watson:     } &    2.222  \\\\\n\\textbf{Prob(Omnibus):} &  0.027 & \\textbf{  Jarque-Bera (JB):  } &    2.387  \\\\\n\\textbf{Skew:}          & -0.174 & \\textbf{  Prob(JB):          } &    0.303  \\\\\n\\textbf{Kurtosis:}      &  1.839 & \\textbf{  Cond. No.          } &     15.9  \\\\\n\\bottomrule\n\\end{tabular}\n%\\caption{OLS Regression Results}\n\\end{center}\n\nNotes: \\newline\n [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."},"metadata":{}}],"execution_count":31},{"id":"5c89ab5e-886b-4650-8bc2-9aa989691899","cell_type":"markdown","source":"# OLS Regression Results\n\n## **Key Takeaways**\n- **R² = 0.941** → The model explains **94.1% of the variance** in the dependent variable (**y**).\n- **Adjusted R² = 0.938** → Indicates a strong model fit even after adjusting for predictors.\n- **F-statistic = 289.0** (p-value = **6.71e-23**) → The model is highly significant.\n- **Log-Likelihood = -98.176**  \n- **Observations = 39**, **Df Model = 2**, **Df Residuals = 36**  \n\n### **Model Coefficients:**\n| Feature          | Coefficient | Std. Error | t-value | p-value | 95% CI (Lower) | 95% CI (Upper) |\n|-----------------|------------|------------|---------|---------|---------------|---------------|\n| **Constant**    | 2.7683     | 0.796      | 3.477   | 0.001   | 1.153         | 4.383         |\n| **Temperature** | -0.7064    | 0.181      | -3.894  | 0.000   | -1.074        | -0.339        |\n| **Temperature²** | 1.8715     | 0.081      | 23.186  | 0.000   | 1.708         | 2.035         |\n\n### **Additional Statistics:**\n- **AIC = 202.4**, **BIC = 207.3** → Lower values indicate a better model fit.\n- **Durbin-Watson = 2.222** → No significant autocorrelation in residuals.\n- **Omnibus = 7.190, Prob(Omnibus) = 0.027** → Some evidence of non-normality in residuals.\n- **Jarque-Bera (JB) = 2.387, Prob(JB) = 0.303** → Residuals show no strong deviation from normality.\n- **Skew = -0.174**, **Kurtosis = 1.839** → Slight left skew, low kurtosis.\n","metadata":{}},{"id":"60fe7500","cell_type":"markdown","source":"## Forward Selection","metadata":{}},{"id":"4fe101ec","cell_type":"code","source":"\ndef forward_selection(X, y, significance_level=0.05):\n    initial_features = []\n    remaining_features = list(X.columns)\n    best_model = None\n    while len(remaining_features) > 0:\n        best_p_value = float(\"inf\")\n        best_feature = None\n        for feature in remaining_features:\n            selected_features = initial_features + [feature]\n            X_opt = sm.add_constant(X[selected_features])\n            model = sm.OLS(y, X_opt).fit()\n            p_value = model.pvalues[feature]\n            if p_value < significance_level and p_value < best_p_value:\n                best_p_value = p_value\n                best_feature = feature\n                best_model = model\n        if best_feature:\n            initial_features.append(best_feature)\n            remaining_features.remove(best_feature)\n        else:\n            break\n    return best_model, initial_features\n\nfs_model, fs_features = forward_selection(X_train_df, y_train)\nfs_model.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T17:30:04.662799Z","iopub.execute_input":"2025-03-10T17:30:04.663228Z","iopub.status.idle":"2025-03-10T17:30:04.725741Z","shell.execute_reply.started":"2025-03-10T17:30:04.663199Z","shell.execute_reply":"2025-03-10T17:30:04.724889Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.941\nModel:                            OLS   Adj. R-squared:                  0.938\nMethod:                 Least Squares   F-statistic:                     289.0\nDate:                Mon, 10 Mar 2025   Prob (F-statistic):           6.71e-23\nTime:                        17:30:04   Log-Likelihood:                -98.176\nNo. Observations:                  39   AIC:                             202.4\nDf Residuals:                      36   BIC:                             207.3\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P>|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nTemperature^2     1.8715      0.081     23.186      0.000       1.708       2.035\nTemperature      -0.7064      0.181     -3.894      0.000      -1.074      -0.339\nconst             2.7683      0.796      3.477      0.001       1.153       4.383\n==============================================================================\nOmnibus:                        7.190   Durbin-Watson:                   2.222\nProb(Omnibus):                  0.027   Jarque-Bera (JB):                2.387\nSkew:                          -0.174   Prob(JB):                        0.303\nKurtosis:                       1.839   Cond. No.                         15.9\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\"\"\"","text/html":"<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.941</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.938</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   289.0</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Mon, 10 Mar 2025</td> <th>  Prob (F-statistic):</th> <td>6.71e-23</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>17:30:04</td>     <th>  Log-Likelihood:    </th> <td> -98.176</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>    39</td>      <th>  AIC:               </th> <td>   202.4</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>    36</td>      <th>  BIC:               </th> <td>   207.3</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n        <td></td>           <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>Temperature^2</th> <td>    1.8715</td> <td>    0.081</td> <td>   23.186</td> <td> 0.000</td> <td>    1.708</td> <td>    2.035</td>\n</tr>\n<tr>\n  <th>Temperature</th>   <td>   -0.7064</td> <td>    0.181</td> <td>   -3.894</td> <td> 0.000</td> <td>   -1.074</td> <td>   -0.339</td>\n</tr>\n<tr>\n  <th>const</th>         <td>    2.7683</td> <td>    0.796</td> <td>    3.477</td> <td> 0.001</td> <td>    1.153</td> <td>    4.383</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td> 7.190</td> <th>  Durbin-Watson:     </th> <td>   2.222</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.027</td> <th>  Jarque-Bera (JB):  </th> <td>   2.387</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td>-0.174</td> <th>  Prob(JB):          </th> <td>   0.303</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 1.839</td> <th>  Cond. No.          </th> <td>    15.9</td>\n</tr>\n</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.","text/latex":"\\begin{center}\n\\begin{tabular}{lclc}\n\\toprule\n\\textbf{Dep. Variable:}    &        y         & \\textbf{  R-squared:         } &     0.941   \\\\\n\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.938   \\\\\n\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     289.0   \\\\\n\\textbf{Date:}             & Mon, 10 Mar 2025 & \\textbf{  Prob (F-statistic):} &  6.71e-23   \\\\\n\\textbf{Time:}             &     17:30:04     & \\textbf{  Log-Likelihood:    } &   -98.176   \\\\\n\\textbf{No. Observations:} &          39      & \\textbf{  AIC:               } &     202.4   \\\\\n\\textbf{Df Residuals:}     &          36      & \\textbf{  BIC:               } &     207.3   \\\\\n\\textbf{Df Model:}         &           2      & \\textbf{                     } &             \\\\\n\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lcccccc}\n                       & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n\\midrule\n\\textbf{Temperature^2} &       1.8715  &        0.081     &    23.186  &         0.000        &        1.708    &        2.035     \\\\\n\\textbf{Temperature}   &      -0.7064  &        0.181     &    -3.894  &         0.000        &       -1.074    &       -0.339     \\\\\n\\textbf{const}         &       2.7683  &        0.796     &     3.477  &         0.001        &        1.153    &        4.383     \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lclc}\n\\textbf{Omnibus:}       &  7.190 & \\textbf{  Durbin-Watson:     } &    2.222  \\\\\n\\textbf{Prob(Omnibus):} &  0.027 & \\textbf{  Jarque-Bera (JB):  } &    2.387  \\\\\n\\textbf{Skew:}          & -0.174 & \\textbf{  Prob(JB):          } &    0.303  \\\\\n\\textbf{Kurtosis:}      &  1.839 & \\textbf{  Cond. No.          } &     15.9  \\\\\n\\bottomrule\n\\end{tabular}\n%\\caption{OLS Regression Results}\n\\end{center}\n\nNotes: \\newline\n [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."},"metadata":{}}],"execution_count":32},{"id":"36edb631-7f3c-419b-a788-205c5c229bd1","cell_type":"markdown","source":"# OLS Regression Results\n\n## **Key Takeaways**\n- **R² = 0.941** → The model explains **94.1% of the variance** in the dependent variable (**y**).\n- **Adjusted R² = 0.938** → Indicates a strong model fit even after adjusting for predictors.\n- **F-statistic = 289.0** (p-value = **6.71e-23**) → The model is statistically significant.\n- **Log-Likelihood = -98.176**\n- **Observations = 39**, **Df Model = 2**, **Df Residuals = 36**\n\n## **Model Coefficients**\n| Feature         | Coefficient | Std. Error | t-value | p-value | 95% CI (Lower) | 95% CI (Upper) |\n|----------------|------------|------------|---------|---------|---------------|---------------|\n| **Temperature²** | 1.8715     | 0.081      | 23.186  | 0.000   | 1.708         | 2.035         |\n| **Temperature** | -0.7064    | 0.181      | -3.894  | 0.000   | -1.074        | -0.339        |\n| **Constant**    | 2.7683     | 0.796      | 3.477   | 0.001   | 1.153         | 4.383         |\n\n## **Additional Statistics**\n- **AIC = 202.4**, **BIC = 207.3** → Lower values indicate a better model fit.\n- **Durbin-Watson = 2.222** → No significant autocorrelation in residuals.\n- **Omnibus = 7.190, Prob(Omnibus) = 0.027** → Some evidence of non-normality in residuals.\n- **Jarque-Bera (JB) = 2.387, Prob(JB) = 0.303** → Residuals show no strong deviation from normality.\n- **Skew = -0.174**, **Kurtosis = 1.839** → Slight left skew, low kurtosis.\n- **Condition Number = 15.9** → No signs of strong multicollinearity.\n\n","metadata":{}},{"id":"4068e66c","cell_type":"markdown","source":"## Bidirectional Selection","metadata":{}},{"id":"4d09a235","cell_type":"code","source":"\ndef bidirectional_selection(X, y, significance_level=0.05):\n    initial_features = []\n    remaining_features = list(X.columns)\n    best_model = None\n    while len(remaining_features) > 0:\n        best_p_value = float(\"inf\")\n        best_feature = None\n        for feature in remaining_features:\n            selected_features = initial_features + [feature]\n            X_opt = sm.add_constant(X[selected_features])\n            model = sm.OLS(y, X_opt).fit()\n            p_value = model.pvalues[feature]\n            if p_value < significance_level and p_value < best_p_value:\n                best_p_value = p_value\n                best_feature = feature\n                best_model = model\n        if best_feature:\n            initial_features.append(best_feature)\n            remaining_features.remove(best_feature)\n            while len(initial_features) > 0:\n                X_opt = sm.add_constant(X[initial_features])\n                model = sm.OLS(y, X_opt).fit()\n                max_p_value = model.pvalues.max()\n                if max_p_value > significance_level:\n                    excluded_feature = model.pvalues.idxmax()\n                    initial_features.remove(excluded_feature)\n                else:\n                    break\n        else:\n            break\n    return best_model, initial_features\n\nbs_model, bs_features = bidirectional_selection(X_train_df, y_train)\nbs_model.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T17:30:09.144007Z","iopub.execute_input":"2025-03-10T17:30:09.144387Z","iopub.status.idle":"2025-03-10T17:30:09.219960Z","shell.execute_reply.started":"2025-03-10T17:30:09.144358Z","shell.execute_reply":"2025-03-10T17:30:09.218863Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.941\nModel:                            OLS   Adj. R-squared:                  0.938\nMethod:                 Least Squares   F-statistic:                     289.0\nDate:                Mon, 10 Mar 2025   Prob (F-statistic):           6.71e-23\nTime:                        17:30:09   Log-Likelihood:                -98.176\nNo. Observations:                  39   AIC:                             202.4\nDf Residuals:                      36   BIC:                             207.3\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P>|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nTemperature^2     1.8715      0.081     23.186      0.000       1.708       2.035\nTemperature      -0.7064      0.181     -3.894      0.000      -1.074      -0.339\nconst             2.7683      0.796      3.477      0.001       1.153       4.383\n==============================================================================\nOmnibus:                        7.190   Durbin-Watson:                   2.222\nProb(Omnibus):                  0.027   Jarque-Bera (JB):                2.387\nSkew:                          -0.174   Prob(JB):                        0.303\nKurtosis:                       1.839   Cond. No.                         15.9\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\"\"\"","text/html":"<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.941</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.938</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   289.0</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Mon, 10 Mar 2025</td> <th>  Prob (F-statistic):</th> <td>6.71e-23</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>17:30:09</td>     <th>  Log-Likelihood:    </th> <td> -98.176</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>    39</td>      <th>  AIC:               </th> <td>   202.4</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>    36</td>      <th>  BIC:               </th> <td>   207.3</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n        <td></td>           <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>Temperature^2</th> <td>    1.8715</td> <td>    0.081</td> <td>   23.186</td> <td> 0.000</td> <td>    1.708</td> <td>    2.035</td>\n</tr>\n<tr>\n  <th>Temperature</th>   <td>   -0.7064</td> <td>    0.181</td> <td>   -3.894</td> <td> 0.000</td> <td>   -1.074</td> <td>   -0.339</td>\n</tr>\n<tr>\n  <th>const</th>         <td>    2.7683</td> <td>    0.796</td> <td>    3.477</td> <td> 0.001</td> <td>    1.153</td> <td>    4.383</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td> 7.190</td> <th>  Durbin-Watson:     </th> <td>   2.222</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.027</td> <th>  Jarque-Bera (JB):  </th> <td>   2.387</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td>-0.174</td> <th>  Prob(JB):          </th> <td>   0.303</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 1.839</td> <th>  Cond. No.          </th> <td>    15.9</td>\n</tr>\n</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.","text/latex":"\\begin{center}\n\\begin{tabular}{lclc}\n\\toprule\n\\textbf{Dep. Variable:}    &        y         & \\textbf{  R-squared:         } &     0.941   \\\\\n\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.938   \\\\\n\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     289.0   \\\\\n\\textbf{Date:}             & Mon, 10 Mar 2025 & \\textbf{  Prob (F-statistic):} &  6.71e-23   \\\\\n\\textbf{Time:}             &     17:30:09     & \\textbf{  Log-Likelihood:    } &   -98.176   \\\\\n\\textbf{No. Observations:} &          39      & \\textbf{  AIC:               } &     202.4   \\\\\n\\textbf{Df Residuals:}     &          36      & \\textbf{  BIC:               } &     207.3   \\\\\n\\textbf{Df Model:}         &           2      & \\textbf{                     } &             \\\\\n\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lcccccc}\n                       & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n\\midrule\n\\textbf{Temperature^2} &       1.8715  &        0.081     &    23.186  &         0.000        &        1.708    &        2.035     \\\\\n\\textbf{Temperature}   &      -0.7064  &        0.181     &    -3.894  &         0.000        &       -1.074    &       -0.339     \\\\\n\\textbf{const}         &       2.7683  &        0.796     &     3.477  &         0.001        &        1.153    &        4.383     \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lclc}\n\\textbf{Omnibus:}       &  7.190 & \\textbf{  Durbin-Watson:     } &    2.222  \\\\\n\\textbf{Prob(Omnibus):} &  0.027 & \\textbf{  Jarque-Bera (JB):  } &    2.387  \\\\\n\\textbf{Skew:}          & -0.174 & \\textbf{  Prob(JB):          } &    0.303  \\\\\n\\textbf{Kurtosis:}      &  1.839 & \\textbf{  Cond. No.          } &     15.9  \\\\\n\\bottomrule\n\\end{tabular}\n%\\caption{OLS Regression Results}\n\\end{center}\n\nNotes: \\newline\n [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."},"metadata":{}}],"execution_count":33},{"id":"be83461b-08ad-4a7a-8ac0-3322d016cd2d","cell_type":"markdown","source":"# **OLS Regression Summary**  \n\n## **Key Metrics**  \n- **R² = 0.941**, Adjusted **R² = 0.938** → Strong model fit  \n- **F-statistic = 289.0** (p = **6.71e-23**) → Model is highly significant  \n- **AIC = 202.4, BIC = 207.3** → Lower is better  \n\n## **Model Coefficients**  \n| Feature       | Coef.  | Std. Err | t-value | p-value |\n|--------------|--------|----------|---------|---------|\n| **Temp²**    | 1.8715 | 0.081    | 23.186  | 0.000   |\n| **Temp**     | -0.7064 | 0.181    | -3.894  | 0.000   |\n| **Constant** | 2.7683 | 0.796    | 3.477   | 0.001   |\n\n## **Diagnostics**  \n- **Durbin-Watson = 2.222** → No strong autocorrelation  \n- **Omnibus p = 0.027** → Mild non-normality in residuals  \n- **Jarque-Bera p = 0.303** → No strong deviation from normality   \n","metadata":{}},{"id":"f4e524b2-6097-4c3c-9fa4-6667f8f223dd","cell_type":"code","source":"X_test_df1 = pd.DataFrame(X_test1, columns=poly1.get_feature_names_out(['Temperature']))\nX_test_df1 = sm.add_constant(X_test_df1)\n\nX_test_df2 = pd.DataFrame(X_test2, columns=poly2.get_feature_names_out(['Temperature']))\nX_test_df2 = sm.add_constant(X_test_df2)\n\ny_pred1 = model1.predict(X_test_df1)\ny_pred2 = model2.predict(X_test_df2)\n\n# Compare R² and Adjusted R²\nmodels = {'Model 1 (Degree 2)': model1, 'Model 2 (Degree 3)': model2}\nfor name, m in models.items():\n    print(f\"{name}: R² = {m.rsquared:.4f}, Adjusted R² = {m.rsquared_adj:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T17:30:32.486673Z","iopub.execute_input":"2025-03-10T17:30:32.486997Z","iopub.status.idle":"2025-03-10T17:30:32.500024Z","shell.execute_reply.started":"2025-03-10T17:30:32.486972Z","shell.execute_reply":"2025-03-10T17:30:32.499005Z"}},"outputs":[{"name":"stdout","text":"Model 1 (Degree 2): R² = 0.9414, Adjusted R² = 0.9381\nModel 2 (Degree 3): R² = 0.9469, Adjusted R² = 0.9424\n","output_type":"stream"}],"execution_count":35},{"id":"0c13cde0-3e94-4b38-b184-74dd9c706ab8","cell_type":"code","source":"# Compare models based on R² and Adjusted R²\nmodels = {'Model 1': model1, 'Model 2': model2}\n\nprint(\"Model Performance Comparison:\")\nfor name, model in models.items():\n    print(f\"\\n{name}:\")\n    print(f\"  - R² Score: {model.rsquared:.4f}\")\n    print(f\"  - Adjusted R² Score: {model.rsquared_adj:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T17:31:36.528602Z","iopub.execute_input":"2025-03-10T17:31:36.528956Z","iopub.status.idle":"2025-03-10T17:31:36.538094Z","shell.execute_reply.started":"2025-03-10T17:31:36.528929Z","shell.execute_reply":"2025-03-10T17:31:36.536815Z"}},"outputs":[{"name":"stdout","text":"Model Performance Comparison:\n\nModel 1:\n  - R² Score: 0.9414\n  - Adjusted R² Score: 0.9381\n\nModel 2:\n  - R² Score: 0.9469\n  - Adjusted R² Score: 0.9424\n","output_type":"stream"}],"execution_count":36},{"id":"37b14cad-5c7f-4f92-8d7f-9d585acac030","cell_type":"markdown","source":"## Model Performance Comparison\n\n### Model 1 (Degree 2)\n- R² Score: 0.9414\n- Adjusted R² Score: 0.9381\n\n### Model 2 (Degree 3)\n- R² Score: 0.9469\n- Adjusted R² Score: 0.9424\n\n### Interpretation\n- **R² Score**: Model 2 explains a higher percentage of variance in the dependent variable (Sales).\n- **Adjusted R²**: Model 2 still performs better even after adjusting for extra predictors.\n\n### Conclusion\nModel 2 (Degree 3) is the better fit, as it captures more variance without overfitting significantly. However, if overfitting is observed, Model 1 (Degree 2) may still be a better choice for generalization.\n","metadata":{}}]}